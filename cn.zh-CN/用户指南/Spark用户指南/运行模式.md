# 运行模式 {#concept_e1h_35c_kgb .concept}

目前MaxCompute Spark支持以下几种运行方式：local模式，cluster模式，和在DataWorks中执行。

## 运行模式 {#section_ybd_fbc_kgb .section}

1.  Local模式

    local模式主要是让用户能够方便的调试应用代码，使用方式跟社区相同，我们添加了用tunnel读写ODPS表的功能。用户可以在ide和命令行中使用该模式，需要添加配置spark.master=local\[N\]，其中N表示执行该模式所需要的cpu资源。此外，local模式下的读写表是通过读写tunnel完成的，需要在Spark-defaults.conf中增加tunnel配置项\(请根据MaxCompute项目所在的region及网络环境填写对应的[Tunnel Endpoint地址](https://help.aliyun.com/document_detail/34951.html)\)：tunnel\_end\_point=http://dt.cn-beijing.maxcompute.aliyun.com。

    命令行执行该模式的方式如下：

    ```language-php
    1.bin/spark-submit --master local[4] --class com.aliyun.odps.spark.examples.SparkPi ./lib/spark-examples-1.0.0-SNAPSHOT-shaded.jar
    ```

2.  Cluster模式

    在Cluster模式中，用户需要指定自定义程序入口Main，Main结束（Success or Fail）spark job就会结束。使用场景适合于离线作业，可以与阿里云DataWorks产品结合进行作业调度。命令行提交方式如下：

    ```language-java
    1.bin/spark-submit --master yarn-cluster –class SparkPi ./lib/spark-examples-1.0.0-SNAPSHOT-shaded.jar
    ```

3.  DataWorks执行模式

    **说明：** DataWorks的Spark节点目前在灰度发布中，如果您有在DataWorks中调用Spark节点的需求，请通过工单或加入钉钉群：21969532\(MaxCompute Spark支持群\)进行沟通、申请。

    用户可以在DataWorks中运行MaxCompute Spark离线作业（cluster模式），以方便与其他类型执行节点集成和调度。

    1.  用户需要在DataWorks的业务流程中上传并提交\(单击**提交**按钮\)资源：

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/154712690336706_zh-CN.png)

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/154712690336708_zh-CN.png)

    2.  第二步：在创建的业务流程中，从**数据开发**组件中选择**ODPS Spark**节点。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/154712690336713_zh-CN.png)

        双击拖拽到工作流的spark节点，对spark作业进行任务定义：

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/154712690336717_zh-CN.png)

        选择Spark的版本、任务使用的开发语言，并指定任务所使用的资源文件。这里的资源文件就是第一步在业务流程中预先上传并发布的资源文件。同时，您还可以指定提交作业时的配置项，如executor的数量、内存大小等配置项。同时设置配置项：`spark.hadoop.odps.cupid.webproxy.endpoint`\(取值填写项目所在region的endpoint，如http://service.cn.maxcompute.aliyun-inc.com/api\)、spark.hadoop.odps.moye.trackurl.host\(取值填写：http://jobview.odps.aliyun.com\)

        以便能够查看日志中打印出的jobview信息。

        手动执行Spark节点，可以查看该任务的执行日志，从打印出来的日志中可以获取该任务的logview和jobview的url，编译进一步查看与诊断。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/154712690336724_zh-CN.png)

        Spark作业定义完成后，即可以在业务流程中对不同类型服务进行编排、统一调度执行。


## PySpark {#section_ckq_l2c_kgb .section}

以一个简单的pyspark读写MaxCompute表作为例子，下面讲解一下整个开发过程：

-   1.下载[odps\_sdk.py](https://github.com/aliyun/aliyun-cupid-sdk/blob/3.3.2-public/spark/spark-2.x/datasource/src/main/python/odps_sdk.py)到本地。
-   2.下载[odps-spark-datasource-1.0.3.jar](http://repo.aliyun.com/download/odps-spark-datasource-1.0.3.jar)至本地，截止至文档发布的时刻，1.0.3为最新版本，如版本更新请自行更新。
-   3.准备python版spark app，如下:
-   4.example.py代码示例：

    ```language-python
    1.from odps.odps_sdk import OdpsOps
    2.from pyspark import SparkContext, SparkConf
    3.from pyspark.sql import SQLContext, DataFrame
    4.
    5.if __name__ == '__main__':
    6.    conf = SparkConf().setAppName("pyspark_odps_test")
    7.    sc = SparkContext(conf=conf)
    8.    sql_context = SQLContext(sc)
    9.
    10.    # Normal ODPS Table READ
    11.    # Params: partitions [default value]
    12.    # Params: num_partition [default value]
    13.    normal_df = OdpsOps.read_odps_table(sql_context, "cupid_testa1", "cupid_wordcount")
    14.    for i in normal_df.sample(False, 0.01).collect():
    15.        print i
    16.    print "Read normal odps table finished"
    17.
    18.    # Partition ODPS Table READ
    19.    # Params: partitions ["pt1='part1',pt2='part1'", "pt1='part1',pt2='part2'"]
    20.    # Params: num_partition [default value]
    21.    partition_df = OdpsOps.read_odps_table(sql_context, "cupid_testa1", "cupid_partition_table1",
    22.                                 partitions=["pt1='part1',pt2='part1'", "pt1='part1',pt2='part2'"])
    23.    for i in partition_df.sample(False, 0.01).collect():
    24.        print i
    25.    print "Read partition odps table finished"
    26.
    27.    # Normal ODPS Table Write
    28.    # Params: partition_str [default value]
    29.    # Params: is_overwrite [default value]
    30.    OdpsOps.write_odps_table(sql_context, normal_df.sample(False, 0.001), "cupid_testa1", "cupid_wordcount_py")
    31.    print "Write normal odps table finished"
    32.
    33.    # Partition ODPS Table Write
    34.    # Params: partition_str "pt1='part3',pt2='part4'"
    35.    # Params: is_overwrite [default value]
    36.    OdpsOps.write_odps_table(sql_context, partition_df.sample(False, 0.001), "cupid_testa1", "cupid_partition_table1_py",
    37.                             "pt1='part1',pt2='part3'")
    38.    print "Write partition odps table finished"
    ```

    Cluster模式运行

    ```language-python
    spark-submit --master yarn-cluster --jars odps-spark-datasource-1.0.3.jar --py-files odps_sdk.py example.py
    ```


